name: docs
on:
  push:
    branches: [main]

permissions:
  contents: write

env:
  DBT_PROFILES_DIR: transform/ci
{% if dbt_target == 'Snowflake' %}
{% raw %}
  DBT_RAW_DB: RAW_PRD
  DBT_ANALYTICS_DB: ANALYTICS_PRD
  SNOWFLAKE_PRIVATE_KEY: ${{ SECRETS.SNOWFLAKE_PRIVATE_KEY_PRD }}
  SNOWFLAKE_USER: ${{ SECRETS.SNOWFLAKE_USER_PRD }}
  SNOWFLAKE_ACCOUNT: ${{ SECRETS.SNOWFLAKE_ACCOUNT }}
{% endraw %}
{% endif %}

jobs:
  build-docs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: "3.10"
{% if dbt_target == 'BigQuery' %}
      - id: auth
        name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v1
        with:
          # To set this up you should create a GCP service account
          # with the "BigQuery Metadata Viewer", "BigQuery Job User", and
          # "BigQuery Data Viewer" roles. This will allow it to execute read-only
          # queries against your data warehouse, which is needed to generate the docs
          # Next, create a service account JSON file, and put it into a GitHub actions
          # secret under the name GOOGLE_CREDENTIALS.
{% raw %}
          credentials_json: ${{ secrets.GOOGLE_CREDENTIALS }}
{% endraw %}
          export_environment_variables: true
{% endif %}
      - uses: actions/cache@v2
        with:
          key: {% raw %}${{ github.ref }}
{% endraw %}
          path: .cache
      - uses: snok/install-poetry@v1
        with:
          virtualenvs-create: false
      - name: Install dependencies
        run: |
          poetry install
      - name: Build dbt docs
        run: |
          dbt deps --project-dir=transform
          dbt docs generate --project-dir=transform --target=prd
          cp -r transform/target docs/dbt_docs
      - name: Deploy docs to GitHub Pages
        if: github.ref.name == 'refs/heads/main'
        run: poetry run mkdocs gh-deploy --force
