name: pre-commit

on:
  pull_request:
  push:
    branches: [main]

env:
  DBT_PROFILES_DIR: transform/ci
{% if dbt_target == 'Snowflake' %}
{% raw %}
  PRIVATE_KEY: ${{ SECRETS.SNOWFLAKE_PRIVATE_KEY }}
  SNOWFLAKE_USER: ${{ SECRETS.SNOWFLAKE_USER }}
  SNOWFLAKE_ACCOUNT: ${{ SECRETS.SNOWFLAKE_ACCOUNT }}
{% endraw %}
  SNOWFLAKE_PRIVATE_KEY_PATH: /tmp/private_key.p8
{% endif %}

defaults:
  run:
    shell: bash -l {0}

jobs:
  pre-commit:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
{% if dbt_target == 'BigQuery' %}
      - id: auth
        name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v1
        with:
          # To set this up you should create a GCP service account
          # with the "BigQuery Metadata Viewer", "BigQuery Job User", and
          # "BigQuery Data Viewer" roles. This will allow it to execute read-only
          # queries against your data warehouse, which is needed to generate the docs
          # Next, create a service account JSON file, and put it into a GitHub actions
          # secret under the name GOOGLE_CREDENTIALS.
{% raw %}
          credentials_json: ${{ secrets.GOOGLE_CREDENTIALS }}
{% endraw %}
          export_environment_variables: true
{% elif dbt_target == 'Snowflake' %}
        # TODO: once we are on dbt-snowflake 1.5, no need to pipe to a file, we can
        # just use $SNOWFLAKE_PRIVATE_KEY
      - name: Set up private key
        run: echo "$PRIVATE_KEY" > $SNOWFLAKE_PRIVATE_KEY_PATH
{% endif %}
      - uses: actions/setup-python@v3
      - uses: pre-commit/action@v3.0.0
